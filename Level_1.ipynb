{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"colab":{"provenance":[]}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## **Imports and Setups**","metadata":{"id":"VYMbHKR8-2x4"}},{"cell_type":"markdown","source":"Скачаем и импортируем необходимые зависимости.","metadata":{"id":"vvJagjiB-2x-"}},{"cell_type":"code","source":"%pip install transformers trl","metadata":{"id":"QLycJJqn-2yA","outputId":"cb63d87f-82ce-49c2-daf0-d7cb02b3c179","execution":{"iopub.status.busy":"2023-12-11T08:46:55.709580Z","iopub.execute_input":"2023-12-11T08:46:55.709863Z","iopub.status.idle":"2023-12-11T08:47:09.401459Z","shell.execute_reply.started":"2023-12-11T08:46:55.709838Z","shell.execute_reply":"2023-12-11T08:47:09.400324Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.35.2)\nCollecting trl\n  Obtaining dependency information for trl from https://files.pythonhosted.org/packages/0d/44/c406c3cf5981bddb16ff72acb5ca235888db4073d868cf51bd143bef3aad/trl-0.7.4-py3-none-any.whl.metadata\n  Downloading trl-0.7.4-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.4)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\nRequirement already satisfied: torch>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from trl) (2.0.0)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (from trl) (0.25.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from trl) (2.1.0)\nCollecting tyro>=0.5.11 (from trl)\n  Obtaining dependency information for tyro>=0.5.11 from https://files.pythonhosted.org/packages/c5/11/abdf67467d06713b431618732a43f82d1b1f02120107b05a789afbcdf54d/tyro-0.6.0-py3-none-any.whl.metadata\n  Downloading tyro-0.6.0-py3-none-any.whl.metadata (7.5 kB)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.12.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.4.0->trl) (3.1.2)\nRequirement already satisfied: docstring-parser>=0.14.1 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (0.15)\nRequirement already satisfied: rich>=11.1.0 in /opt/conda/lib/python3.10/site-packages (from tyro>=0.5.11->trl) (13.5.2)\nCollecting shtab>=1.5.6 (from tyro>=0.5.11->trl)\n  Obtaining dependency information for shtab>=1.5.6 from https://files.pythonhosted.org/packages/40/ad/7227da64498eaa7abecee4311008f70869e156014b3270cec36e2e70cd31/shtab-1.6.5-py3-none-any.whl.metadata\n  Downloading shtab-1.6.5-py3-none-any.whl.metadata (7.3 kB)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate->trl) (5.9.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (2.0.3)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.70.15)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (3.8.5)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets->trl) (0.18.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2023.11.17)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->trl) (1.3.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.16.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->trl) (2.1.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->trl) (2023.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.4.0->trl) (1.3.0)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\nDownloading trl-0.7.4-py3-none-any.whl (133 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tyro-0.6.0-py3-none-any.whl (100 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.9/100.9 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading shtab-1.6.5-py3-none-any.whl (13 kB)\nInstalling collected packages: shtab, tyro, trl\nSuccessfully installed shtab-1.6.5 trl-0.7.4 tyro-0.6.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\nimport pandas as pd\nimport numpy as np\nimport random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom dataclasses import dataclass, field\nfrom typing import Dict, Optional\n\ntqdm.pandas()\n\nfrom transformers import (\n    pipeline,\n    AutoTokenizer,\n    TrainingArguments,\n    AutoModelForCausalLM\n)\n\nimport datasets\n\nfrom torch.utils.data import (\n    Dataset,\n    DataLoader,\n    RandomSampler,\n    random_split\n)\n\nfrom trl import (\n    AutoModelForCausalLMWithValueHead,\n    DPOTrainer,\n    create_reference_model\n)\n\nfrom trl.core import LengthSampler\n\nfrom typing import List, Dict\nfrom scipy.stats import entropy\nfrom collections import defaultdict\n\nimport wandb\n\nimport pickle\n\nimport gc","metadata":{"id":"7S5_fBFd-2yH","outputId":"18a4aa01-98fd-4da9-d3a7-04b427491f5e","execution":{"iopub.status.busy":"2023-12-11T08:47:09.403748Z","iopub.execute_input":"2023-12-11T08:47:09.404147Z","iopub.status.idle":"2023-12-11T08:47:27.667738Z","shell.execute_reply.started":"2023-12-11T08:47:09.404112Z","shell.execute_reply":"2023-12-11T08:47:27.666827Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **Configuration and Seed**","metadata":{"id":"9v6iCO6K-2yI"}},{"cell_type":"markdown","source":"Зададим параметры для генерации, оценки и т.д. В качестве SFT модели возьмем gpt2, обученную на imdb датасете https://huggingface.co/lvwerra/gpt2-imdb. Также зафиксируем seed для воспроизводимости результатов.","metadata":{"id":"_UP-cGdE-2yJ"}},{"cell_type":"code","source":"# название SFT модели\nmodel_name = 'lvwerra/gpt2-imdb'\n\n# название Reward модели\nreward_model_name = 'lvwerra/distilbert-imdb'\n\n# задаем параметры оценки\nsentiment_pipe_kwargs = {\n    'top_k': None,\n    'function_to_apply': 'none',\n    'batch_size': 16\n}\n\n# задаем параметры генерации\ngeneration_pipe_kwargs = {\n    'num_return_sequences': 1,\n    'min_length': 32,\n    'max_length': 64,\n    'top_k': 0.0,\n    'top_p': 1.0,\n    'do_sample': True,\n}","metadata":{"id":"hEgU81D6-2yK","execution":{"iopub.status.busy":"2023-12-11T08:47:27.668934Z","iopub.execute_input":"2023-12-11T08:47:27.669213Z","iopub.status.idle":"2023-12-11T08:47:27.674870Z","shell.execute_reply.started":"2023-12-11T08:47:27.669187Z","shell.execute_reply":"2023-12-11T08:47:27.673859Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"def seed_all(seed: int) -> None:\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    random.seed(seed)","metadata":{"id":"iFOXyeFn-2yL","execution":{"iopub.status.busy":"2023-12-11T08:47:27.677384Z","iopub.execute_input":"2023-12-11T08:47:27.677729Z","iopub.status.idle":"2023-12-11T08:47:27.692540Z","shell.execute_reply.started":"2023-12-11T08:47:27.677696Z","shell.execute_reply":"2023-12-11T08:47:27.691696Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"seed_all(42)","metadata":{"id":"80F0jcrP-2yM","execution":{"iopub.status.busy":"2023-12-11T08:47:27.693707Z","iopub.execute_input":"2023-12-11T08:47:27.694267Z","iopub.status.idle":"2023-12-11T08:47:27.705252Z","shell.execute_reply.started":"2023-12-11T08:47:27.694239Z","shell.execute_reply":"2023-12-11T08:47:27.704363Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"id":"bi1obyBC-2yN","execution":{"iopub.status.busy":"2023-12-11T08:47:27.706430Z","iopub.execute_input":"2023-12-11T08:47:27.706688Z","iopub.status.idle":"2023-12-11T08:47:27.734085Z","shell.execute_reply.started":"2023-12-11T08:47:27.706666Z","shell.execute_reply":"2023-12-11T08:47:27.733362Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"wandb.init()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T08:47:27.735061Z","iopub.execute_input":"2023-12-11T08:47:27.735294Z","iopub.status.idle":"2023-12-11T08:49:29.558125Z","shell.execute_reply.started":"2023-12-11T08:47:27.735273Z","shell.execute_reply":"2023-12-11T08:49:29.557302Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20231211_084858-kktc4b0l</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/lavrenov-vv/uncategorized/runs/kktc4b0l' target=\"_blank\">bumbling-durian-10</a></strong> to <a href='https://wandb.ai/lavrenov-vv/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/lavrenov-vv/uncategorized' target=\"_blank\">https://wandb.ai/lavrenov-vv/uncategorized</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/lavrenov-vv/uncategorized/runs/kktc4b0l' target=\"_blank\">https://wandb.ai/lavrenov-vv/uncategorized/runs/kktc4b0l</a>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lavrenov-vv/uncategorized/runs/kktc4b0l?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7a44151659c0>"},"metadata":{}}]},{"cell_type":"markdown","source":"## **Level 1**","metadata":{"id":"921SDCm2-2yO"}},{"cell_type":"markdown","source":"Сперва научимся генерировать тексты, с помощью этой модели. Для этого создадим модель, токенайзер и пайплайн текстовой генерации.","metadata":{"id":"WU9WFHVd-2yP"}},{"cell_type":"code","source":"# создаем модель и токенизатор\ngpt2_model = AutoModelForCausalLM.from_pretrained(model_name)\ngpt2_tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left')\n\n# установливем параметр pad_token и pad_token_id\ngpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\ngeneration_pipe_kwargs['pad_token_id'] = gpt2_tokenizer.eos_token_id","metadata":{"id":"SMH5VcHl-2yP","execution":{"iopub.status.busy":"2023-12-11T08:49:29.561506Z","iopub.execute_input":"2023-12-11T08:49:29.561835Z","iopub.status.idle":"2023-12-11T08:49:48.247444Z","shell.execute_reply.started":"2023-12-11T08:49:29.561803Z","shell.execute_reply":"2023-12-11T08:49:48.246476Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/577 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68b22a8f2ca341778150a861c276a8af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b662844d190349c0981adadd0e97f67c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/17.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87949ed46aa94da98e6c02803c4f25ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78eca40c89e649f9ba2e0450ae9aaa53"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c1c414fa2a44cefa6485a179007fdf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a4cd874873b4b63b4c6ba2d12fbf778"}},"metadata":{}}]},{"cell_type":"code","source":"# создаем пайплайн генерации\ntext_generation = pipeline('text-generation', model=model_name, device=device, tokenizer=gpt2_tokenizer, **generation_pipe_kwargs)\n\nprefix_text = '<|startoftext|>'\npostfix_text = '<|endoftext|>'\ngenerated_text = text_generation(prefix_text)[0]\ngenerated_text['generated_text'][len(prefix_text):]","metadata":{"id":"UJnBkl9P-2yQ","outputId":"19817ee7-0319-4f4c-b8c7-ba1a5910f22b","execution":{"iopub.status.busy":"2023-12-11T08:49:48.248871Z","iopub.execute_input":"2023-12-11T08:49:48.249514Z","iopub.status.idle":"2023-12-11T08:49:58.624152Z","shell.execute_reply.started":"2023-12-11T08:49:48.249476Z","shell.execute_reply":"2023-12-11T08:49:58.623091Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'just to be sure.<br /><br />It would have been fun to have seen all this with the help of the Aussie Turner Classic Movies for a mere rating (mine is 2), but alas *I* miss Mr Bates\\' \"bunny parade\" feature \"Tie'"},"metadata":{}}]},{"cell_type":"markdown","source":"Отлично! Получилось сгенерировать текст, перейдем к следующему этапу: сгенерирем N текстов с помощью sft модели, посчитаем reward для каждого\nс помощью https://huggingface.co/lvwerra/distilbert-imdb. Логиты бинарного\nклассификатора можно использовать в качестве значения reward. Больше\nзначение логита — более позитивный текст.","metadata":{"id":"mMcTD6Q2-2yR"}},{"cell_type":"code","source":"sentiment_pipe = pipeline(model='lvwerra/distilbert-imdb', device=device,  **sentiment_pipe_kwargs)","metadata":{"id":"5hcGhIM0-2yS","execution":{"iopub.status.busy":"2023-12-11T08:49:58.629073Z","iopub.execute_input":"2023-12-11T08:49:58.629545Z","iopub.status.idle":"2023-12-11T08:50:19.100296Z","shell.execute_reply.started":"2023-12-11T08:49:58.629507Z","shell.execute_reply":"2023-12-11T08:50:19.099273Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/735 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db1dfcf0d5844c3b9107ff12b32474ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"75a0fd21ea5a4854836f40e7e4a842b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/333 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e36b4f5bb1834aed8e0eddbf485f8488"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd07b8bfe1ab49dd833fe4bb69237cbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e70710948b6240f99bba12148d327349"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca7b855fec8b4cc398df60a05e63cfe0"}},"metadata":{}}]},{"cell_type":"code","source":"def extract_sentpipe_output(outputs):\n    positive_logit = 0\n    for out in outputs:\n        for element in out:\n            if element[\"label\"] == \"POSITIVE\":\n                positive_logit = element[\"score\"]\n    return positive_logit","metadata":{"id":"f24G-ER3-2yT","execution":{"iopub.status.busy":"2023-12-11T08:50:19.101694Z","iopub.execute_input":"2023-12-11T08:50:19.102041Z","iopub.status.idle":"2023-12-11T08:50:19.108904Z","shell.execute_reply.started":"2023-12-11T08:50:19.102007Z","shell.execute_reply":"2023-12-11T08:50:19.107447Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def example_generator(N):\n    example_lst = []\n    for i in range(N):\n        res = {}\n        generated_text = text_generation(prefix_text)[0]['generated_text'][len(prefix_text):]\n        labels = sentiment_pipe(generated_text)\n        res['generated_text'] = generated_text\n        res['reward'] = extract_sentpipe_output(labels)\n        example_lst.append(res)\n    return example_lst","metadata":{"id":"UDhLsGer-2yT","execution":{"iopub.status.busy":"2023-12-11T08:50:19.110099Z","iopub.execute_input":"2023-12-11T08:50:19.110427Z","iopub.status.idle":"2023-12-11T08:50:19.119789Z","shell.execute_reply.started":"2023-12-11T08:50:19.110395Z","shell.execute_reply":"2023-12-11T08:50:19.118795Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Примем N = 5, сгенерированные примеры и скор отобразим в виде датафрэйма.","metadata":{"id":"XRxRf2zP-2yU"}},{"cell_type":"code","source":"N = 5\ndf = pd.DataFrame(example_generator(N))\ndf","metadata":{"id":"q0zw3MvY-2yV","outputId":"75c0a228-c0ac-4894-8a2e-41ad7828b116","execution":{"iopub.status.busy":"2023-12-11T08:50:19.121091Z","iopub.execute_input":"2023-12-11T08:50:19.121422Z","iopub.status.idle":"2023-12-11T08:50:22.003354Z","shell.execute_reply.started":"2023-12-11T08:50:19.121390Z","shell.execute_reply":"2023-12-11T08:50:22.002164Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"                                      generated_text    reward\n0  presumably tellingThings.dd. Ty Cobb had Kim P... -1.217334\n1  <br /><br />The movie is stellar; it's quite a...  2.677276\n2   =><br />0:27 <|beginpoint of text| startoflin... -0.759771\n3   <p>The addition of \" Also Based on Annis Berg... -2.091571\n4  And satisfy yourself, eat takeout.<br /><br />...  0.515937","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>generated_text</th>\n      <th>reward</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>presumably tellingThings.dd. Ty Cobb had Kim P...</td>\n      <td>-1.217334</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;br /&gt;&lt;br /&gt;The movie is stellar; it's quite a...</td>\n      <td>2.677276</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>=&gt;&lt;br /&gt;0:27 &lt;|beginpoint of text| startoflin...</td>\n      <td>-0.759771</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;p&gt;The addition of \" Also Based on Annis Berg...</td>\n      <td>-2.091571</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>And satisfy yourself, eat takeout.&lt;br /&gt;&lt;br /&gt;...</td>\n      <td>0.515937</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Создадим датасет из пар winner-loser.** ","metadata":{"id":"bs4HroQH-2yY"}},{"cell_type":"markdown","source":"Зададим параметры и создадим функцию, которая будет генерировать промпт заданной длины","metadata":{"id":"j2G7os2XH853"}},{"cell_type":"code","source":"# параметры для генерации ответов на промпт\ngeneration_pipe_kwargs_chosen_reject = {\n    'num_return_sequences': 15,\n    'min_length': 64,\n    'max_length': 96,\n    'top_k': 0.0,\n    'top_p': 1.0,\n    'do_sample': True,\n    'pad_token_id': gpt2_tokenizer.eos_token_id\n}","metadata":{"id":"uZ0deE3qL86C","execution":{"iopub.status.busy":"2023-12-11T08:50:22.004726Z","iopub.execute_input":"2023-12-11T08:50:22.005015Z","iopub.status.idle":"2023-12-11T08:50:22.010838Z","shell.execute_reply.started":"2023-12-11T08:50:22.004983Z","shell.execute_reply":"2023-12-11T08:50:22.009462Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def dataset_builder(config, input_min_text_length=32, input_max_text_length=64):\n    prompt_chosen_rejected_dict = {'prompt': [], 'chosen': [], 'rejected': []}\n    output_min_length = input_min_text_length\n    output_max_length = input_max_text_length\n    output_length_sampler = LengthSampler(output_min_length, output_max_length)\n    prompt_generation = pipeline('text-generation', model=model_name, device=device, tokenizer=gpt2_tokenizer, **generation_pipe_kwargs)\n    answer_generation = pipeline('text-generation', model=model_name, device=device, tokenizer=gpt2_tokenizer, **generation_pipe_kwargs_chosen_reject)\n\n    for i in tqdm(range(1000)):\n        gen_len = output_length_sampler()\n        prompt = prompt_generation(prefix_text)[0]['generated_text'][: (len(prefix_text) + gen_len)]\n        texts_generation = answer_generation(prompt)\n        texts = [elem['generated_text'][len(prompt): (len(prompt) + gen_len)] for elem in texts_generation]\n        vocab = {}\n        for text in texts:\n            vocab[text] = extract_sentpipe_output(sentiment_pipe(text))\n        lst = list(dict(sorted(vocab.items(), key=lambda x: x[1], reverse=True)).keys())\n        prompt_chosen_rejected_dict['prompt'].extend([prompt] * 5)\n        prompt_chosen_rejected_dict['chosen'].extend(lst[: 5])\n        prompt_chosen_rejected_dict['rejected'].extend(lst[-5: ])\n    return prompt_chosen_rejected_dict","metadata":{"id":"APRhMwRq-2ye","execution":{"iopub.status.busy":"2023-12-11T08:50:22.012064Z","iopub.execute_input":"2023-12-11T08:50:22.012418Z","iopub.status.idle":"2023-12-11T08:50:22.026870Z","shell.execute_reply.started":"2023-12-11T08:50:22.012387Z","shell.execute_reply":"2023-12-11T08:50:22.025964Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"prompt_chosen_rejected_dict = dataset_builder(generation_pipe_kwargs_chosen_reject)","metadata":{"id":"4lX_DJxv-2yf","outputId":"01263ccd-75cd-4d41-940a-07f3ecafa051","execution":{"iopub.status.busy":"2023-12-11T08:50:22.027948Z","iopub.execute_input":"2023-12-11T08:50:22.028290Z","iopub.status.idle":"2023-12-11T09:16:20.849084Z","shell.execute_reply.started":"2023-12-11T08:50:22.028259Z","shell.execute_reply":"2023-12-11T09:16:20.848125Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"  0%|          | 0/1000 [00:00<?, ?it/s]/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n100%|██████████| 1000/1000 [25:54<00:00,  1.55s/it]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Проверим, что в полученных списка отсутствуют пустые строки","metadata":{}},{"cell_type":"code","source":"assert all(map(lambda x: x != '', prompt_chosen_rejected_dict['prompt']))\nassert all(map(lambda x: x != '', prompt_chosen_rejected_dict['chosen']))\nassert all(map(lambda x: x != '', prompt_chosen_rejected_dict['rejected']))","metadata":{"id":"u5lQJB0uuGVc","outputId":"12285b4e-34b8-4b34-af48-2a847e8d77e0","execution":{"iopub.status.busy":"2023-12-11T09:16:20.850291Z","iopub.execute_input":"2023-12-11T09:16:20.850942Z","iopub.status.idle":"2023-12-11T09:16:20.858543Z","shell.execute_reply.started":"2023-12-11T09:16:20.850906Z","shell.execute_reply":"2023-12-11T09:16:20.857590Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"Взглянем на примеры:","metadata":{}},{"cell_type":"code","source":"for i in range(5):\n    print(prompt_chosen_rejected_dict['prompt'][i], '|', prompt_chosen_rejected_dict['chosen'][i], '|', prompt_chosen_rejected_dict['rejected'][i])","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:16:20.859926Z","iopub.execute_input":"2023-12-11T09:16:20.860487Z","iopub.status.idle":"2023-12-11T09:16:20.874704Z","shell.execute_reply.started":"2023-12-11T09:16:20.860449Z","shell.execute_reply":"2023-12-11T09:16:20.873354Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"<|startoftext|>the longest movie I have seen by much  | irl. Excellent movie, no mean feat ple | ______________________________________\n<|startoftext|>the longest movie I have seen by much  | !! <br /><br />Chris Bachman!!!!!<br / | _________. Well, that's saying somethi\n<|startoftext|>the longest movie I have seen by much  | !!! Rainbow Girl and Underworld 3 were | !!<br /><br />no plot changes but just\n<|startoftext|>the longest movie I have seen by much  | !!<br /><br />I would rate the film lo | !!<br /><br />Do not buy this movie, d\n<|startoftext|>the longest movie I have seen by much  | !!! my opinion is that anything that i | imec utterly fails to represent the gi\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Скачаем полученный датасет:","metadata":{}},{"cell_type":"code","source":"with open(\"prompt_chosen_rejected_dict.pkl\", \"wb\") as file:\n    pickle.dump(prompt_chosen_rejected_dict, file)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:16:20.875548Z","iopub.execute_input":"2023-12-11T09:16:20.875801Z","iopub.status.idle":"2023-12-11T09:16:20.884871Z","shell.execute_reply.started":"2023-12-11T09:16:20.875778Z","shell.execute_reply":"2023-12-11T09:16:20.883881Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Настроим тренировочные параметры.","metadata":{}},{"cell_type":"code","source":"size = len(prompt_chosen_rejected_dict['rejected'])\nsize","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:16:20.886033Z","iopub.execute_input":"2023-12-11T09:16:20.886305Z","iopub.status.idle":"2023-12-11T09:16:20.893834Z","shell.execute_reply.started":"2023-12-11T09:16:20.886272Z","shell.execute_reply":"2023-12-11T09:16:20.892747Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"5000"},"metadata":{}}]},{"cell_type":"markdown","source":"Разделим датасет на выборки.","metadata":{}},{"cell_type":"code","source":"train_split = {\n    'prompt': prompt_chosen_rejected_dict['prompt'][: int(size*0.8)],\n    'chosen': prompt_chosen_rejected_dict['chosen'][: int(size*0.8)],\n    'rejected': prompt_chosen_rejected_dict['rejected'][: int(size*0.8)]\n}\n\neval_split = {\n    'prompt': prompt_chosen_rejected_dict['prompt'][int(size*0.8): int(size*0.95)],\n    'chosen': prompt_chosen_rejected_dict['chosen'][int(size*0.8): int(size*0.95)],\n    'rejected': prompt_chosen_rejected_dict['rejected'][int(size*0.8): int(size*0.95)]\n}\n\ntest_split = {\n    'prompt': prompt_chosen_rejected_dict['prompt'][int(size*0.95): ],\n    'chosen': prompt_chosen_rejected_dict['chosen'][int(size*0.95): ],\n    'rejected': prompt_chosen_rejected_dict['rejected'][int(size*0.95): ]\n}","metadata":{"id":"wzBrekw6tYTa","execution":{"iopub.status.busy":"2023-12-11T09:16:20.895056Z","iopub.execute_input":"2023-12-11T09:16:20.895426Z","iopub.status.idle":"2023-12-11T09:16:20.905894Z","shell.execute_reply.started":"2023-12-11T09:16:20.895392Z","shell.execute_reply":"2023-12-11T09:16:20.904886Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"train_dataset = datasets.Dataset.from_dict(train_split)\neval_dataset = datasets.Dataset.from_dict(eval_split)\ntest_dataset = datasets.Dataset.from_dict(test_split)","metadata":{"id":"AqRbEpDWtcra","execution":{"iopub.status.busy":"2023-12-11T09:16:20.907087Z","iopub.execute_input":"2023-12-11T09:16:20.907374Z","iopub.status.idle":"2023-12-11T09:16:20.937343Z","shell.execute_reply.started":"2023-12-11T09:16:20.907346Z","shell.execute_reply":"2023-12-11T09:16:20.936372Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Получили следующие размеры выборок: ","metadata":{}},{"cell_type":"code","source":"len(train_split['rejected']), len(eval_split['rejected']), len(test_split['rejected'])","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:16:20.938542Z","iopub.execute_input":"2023-12-11T09:16:20.939199Z","iopub.status.idle":"2023-12-11T09:16:20.948297Z","shell.execute_reply.started":"2023-12-11T09:16:20.939166Z","shell.execute_reply":"2023-12-11T09:16:20.947232Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"(4000, 750, 250)"},"metadata":{}}]},{"cell_type":"code","source":"train_kwargs = {\n    'model_name': 'lvwerra/gpt2-imdb',\n    'report_to': 'wandb',\n    'learning_rate': 1e-3,\n    'per_device_train_batch_size': 16,\n    'max_length': 512,\n    'max_steps': 1000,\n    'gradient_accumulation_steps': 1,\n    'beta': 0.1,\n    'max_target_length': 128,\n    'max_prompt_length': 128\n}","metadata":{"id":"u9gRDimDtXd4","execution":{"iopub.status.busy":"2023-12-11T09:16:20.949275Z","iopub.execute_input":"2023-12-11T09:16:20.949624Z","iopub.status.idle":"2023-12-11T09:16:20.959186Z","shell.execute_reply.started":"2023-12-11T09:16:20.949583Z","shell.execute_reply":"2023-12-11T09:16:20.958304Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"hinge_training_args = TrainingArguments(\n        per_device_train_batch_size=train_kwargs['per_device_train_batch_size'],\n        max_steps=train_kwargs['max_steps'],\n        remove_unused_columns=False,\n        gradient_accumulation_steps=train_kwargs['gradient_accumulation_steps'],\n        learning_rate=train_kwargs['learning_rate'],\n        evaluation_strategy='steps',\n        logging_first_step=True,\n        logging_steps=10,\n        eval_steps=500,\n        output_dir='./test1',\n        warmup_steps=100,\n        report_to=train_kwargs['report_to'],\n        gradient_checkpointing=False,\n    )","metadata":{"id":"sGxiOuUCtej8","execution":{"iopub.status.busy":"2023-12-11T09:16:20.960577Z","iopub.execute_input":"2023-12-11T09:16:20.960987Z","iopub.status.idle":"2023-12-11T09:16:20.972458Z","shell.execute_reply.started":"2023-12-11T09:16:20.960949Z","shell.execute_reply":"2023-12-11T09:16:20.971658Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Создадим референсную модель, которую будет необходимо передать в качестве аргумента для DPOTrainer.","metadata":{}},{"cell_type":"code","source":"gpt2_ref_model = create_reference_model(gpt2_model)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:16:20.973739Z","iopub.execute_input":"2023-12-11T09:16:20.974004Z","iopub.status.idle":"2023-12-11T09:16:21.111362Z","shell.execute_reply.started":"2023-12-11T09:16:20.973981Z","shell.execute_reply":"2023-12-11T09:16:21.110340Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"hinge_dpo_trainer = DPOTrainer(\n        gpt2_model,\n        gpt2_ref_model,\n        args=hinge_training_args,\n        beta=train_kwargs['beta'],\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=gpt2_tokenizer,\n        max_length=train_kwargs['max_length'],\n        max_target_length=train_kwargs['max_target_length'],\n        max_prompt_length=train_kwargs['max_prompt_length'],\n        generate_during_eval=True,\n        loss_type='hinge'\n    )","metadata":{"id":"68ZsCd7EWpCh","execution":{"iopub.status.busy":"2023-12-11T09:16:21.112644Z","iopub.execute_input":"2023-12-11T09:16:21.112982Z","iopub.status.idle":"2023-12-11T09:16:21.390529Z","shell.execute_reply.started":"2023-12-11T09:16:21.112949Z","shell.execute_reply":"2023-12-11T09:16:21.389367Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"hinge_dpo_trainer.train()","metadata":{"id":"Aqsh4oHatl6C","execution":{"iopub.status.busy":"2023-12-11T09:16:21.397346Z","iopub.execute_input":"2023-12-11T09:16:21.397630Z","iopub.status.idle":"2023-12-11T09:27:24.467860Z","shell.execute_reply.started":"2023-12-11T09:16:21.397605Z","shell.execute_reply":"2023-12-11T09:27:24.466924Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stderr","text":"Could not estimate the number of tokens of the input, floating-point operations will not be computed\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 11:02, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rewards/chosen</th>\n      <th>Rewards/rejected</th>\n      <th>Rewards/accuracies</th>\n      <th>Rewards/margins</th>\n      <th>Logps/rejected</th>\n      <th>Logps/chosen</th>\n      <th>Logits/rejected</th>\n      <th>Logits/chosen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.261100</td>\n      <td>0.681446</td>\n      <td>-9.743262</td>\n      <td>-13.459870</td>\n      <td>0.803191</td>\n      <td>3.716607</td>\n      <td>-200.227936</td>\n      <td>-160.909714</td>\n      <td>-8.631719</td>\n      <td>-8.566963</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.008000</td>\n      <td>0.666728</td>\n      <td>-14.067310</td>\n      <td>-19.820368</td>\n      <td>0.839096</td>\n      <td>5.753059</td>\n      <td>-263.832947</td>\n      <td>-204.150177</td>\n      <td>-16.603994</td>\n      <td>-16.472866</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.26716122599318626, metrics={'train_runtime': 662.6885, 'train_samples_per_second': 24.144, 'train_steps_per_second': 1.509, 'total_flos': 0.0, 'train_loss': 0.26716122599318626, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"code","source":"hinge_dpo_trainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:27:24.469179Z","iopub.execute_input":"2023-12-11T09:27:24.469481Z","iopub.status.idle":"2023-12-11T09:27:25.319394Z","shell.execute_reply.started":"2023-12-11T09:27:24.469455Z","shell.execute_reply":"2023-12-11T09:27:25.318005Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Оценим reward до обучения и после.","metadata":{}},{"cell_type":"code","source":"bs = len(test_dataset) // 10\nhinge_game_data = dict()\ntest_dataset.set_format('pandas')\ndf_hinge_batch = test_dataset[:].sample(bs)\nhinge_game_data['prompt'] = df_hinge_batch['prompt'].tolist()\nquery_tensors = df_hinge_batch['prompt'].map(gpt2_tokenizer).tolist()\noutput_min_length = 32\noutput_max_length = 64\noutput_length_sampler = LengthSampler(output_min_length, output_max_length)\nresponse_tensors_ref, response_tensors = [], []\n\nfor i in tqdm(range(bs)):\n    gen_len = output_length_sampler()\n    output = gpt2_ref_model.generate(\n        torch.tensor(query_tensors[i]['input_ids']).unsqueeze(dim=0).to(device), **generation_pipe_kwargs).squeeze()[-gen_len:]\n    response_tensors_ref.append(output)\n    output = gpt2_model.generate(\n        torch.tensor(query_tensors[i]['input_ids']).unsqueeze(dim=0).to(device), **generation_pipe_kwargs).squeeze()[-gen_len:]\n    response_tensors.append(output)\n\nhinge_game_data['response (before)'] = [gpt2_tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\nhinge_game_data['response (after)'] = [gpt2_tokenizer.decode(response_tensors[i]) for i in range(bs)]\n\nhinge_texts = [q + r for q, r in zip(hinge_game_data['prompt'], hinge_game_data['response (before)'])]\nhinge_game_data['rewards (before)'] = [output[0]['score'] if output[0]['label'] == 'POSITIVE' else output[1]['score'] for output in sentiment_pipe(hinge_texts,  **sentiment_pipe_kwargs)]\n\nhinge_texts = [q + r for q, r in zip(hinge_game_data['prompt'], hinge_game_data['response (after)'])]\nhinge_game_data['rewards (after)'] = [output[0]['score'] if output[0]['label'] == 'POSITIVE' else output[1]['score'] for output in sentiment_pipe(hinge_texts,  **sentiment_pipe_kwargs)]\n\ndf_hinge_results = pd.DataFrame(hinge_game_data)\ndf_hinge_results.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:27:25.320881Z","iopub.execute_input":"2023-12-11T09:27:25.321619Z","iopub.status.idle":"2023-12-11T09:27:49.477179Z","shell.execute_reply.started":"2023-12-11T09:27:25.321579Z","shell.execute_reply":"2023-12-11T09:27:49.476208Z"},"trusted":true},"execution_count":30,"outputs":[{"name":"stderr","text":"100%|██████████| 25/25 [00:23<00:00,  1.04it/s]\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n","output_type":"stream"},{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  <|startoftext|> partying over mountain suds he...   \n1  <|startoftext|>can't wait to see what happens ...   \n2  <|startoftext|>, say, Phil and Houry're not my fr   \n3  <|startoftext|>4 K&K<br /><br />Battlegate and...   \n4  <|startoftext|> 23rd scm cast <br /><br />goth...   \n\n                                   response (before)  \\\n0  's golden year). no Christmas night <br /><br ...   \n1   /><br />I've always loved the humor--especial...   \n2   can't really take the position that these are...   \n3  <|startoftext|>4 K&K<br /><br />Battlegate and...   \n4  thacha >)<br /><br /> <br /><br />DVD conducte...   \n\n                                    response (after)  rewards (before)  \\\n0   Hall-I We%> TV<>ian :>@ most Gl coversWE=>www...         -2.771453   \n1  /ap>// Once>http:AL> Feast,/and| 7 a <fi LA fa...          1.788933   \n2   comedy comedy 3 (www; Sweetagap/You Do (iv31 ...         -0.468038   \n3  |startoftext|>4 K&K<br /><br />Battlegate and ...          0.636828   \n4  ini-\")23fiI/:ap> 10 I--- ALL andI on myo grand...         -0.463700   \n\n   rewards (after)  \n0         1.294239  \n1         1.475759  \n2        -0.415085  \n3        -0.383679  \n4        -0.072108  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>response (before)</th>\n      <th>response (after)</th>\n      <th>rewards (before)</th>\n      <th>rewards (after)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;|startoftext|&gt; partying over mountain suds he...</td>\n      <td>'s golden year). no Christmas night &lt;br /&gt;&lt;br ...</td>\n      <td>Hall-I We%&gt; TV&lt;&gt;ian :&gt;@ most Gl coversWE=&gt;www...</td>\n      <td>-2.771453</td>\n      <td>1.294239</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;|startoftext|&gt;can't wait to see what happens ...</td>\n      <td>/&gt;&lt;br /&gt;I've always loved the humor--especial...</td>\n      <td>/ap&gt;// Once&gt;http:AL&gt; Feast,/and| 7 a &lt;fi LA fa...</td>\n      <td>1.788933</td>\n      <td>1.475759</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;|startoftext|&gt;, say, Phil and Houry're not my fr</td>\n      <td>can't really take the position that these are...</td>\n      <td>comedy comedy 3 (www; Sweetagap/You Do (iv31 ...</td>\n      <td>-0.468038</td>\n      <td>-0.415085</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;|startoftext|&gt;4 K&amp;K&lt;br /&gt;&lt;br /&gt;Battlegate and...</td>\n      <td>&lt;|startoftext|&gt;4 K&amp;K&lt;br /&gt;&lt;br /&gt;Battlegate and...</td>\n      <td>|startoftext|&gt;4 K&amp;K&lt;br /&gt;&lt;br /&gt;Battlegate and ...</td>\n      <td>0.636828</td>\n      <td>-0.383679</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;|startoftext|&gt; 23rd scm cast &lt;br /&gt;&lt;br /&gt;goth...</td>\n      <td>thacha &gt;)&lt;br /&gt;&lt;br /&gt; &lt;br /&gt;&lt;br /&gt;DVD conducte...</td>\n      <td>ini-\")23fiI/:ap&gt; 10 I--- ALL andI on myo grand...</td>\n      <td>-0.463700</td>\n      <td>-0.072108</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print('mean:')\ndisplay(df_hinge_results[['rewards (before)', 'rewards (after)']].mean())\nprint()\nprint('median:')\ndisplay(df_hinge_results[['rewards (before)', 'rewards (after)']].median())","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:27:49.478341Z","iopub.execute_input":"2023-12-11T09:27:49.478645Z","iopub.status.idle":"2023-12-11T09:27:49.499412Z","shell.execute_reply.started":"2023-12-11T09:27:49.478619Z","shell.execute_reply":"2023-12-11T09:27:49.498483Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"mean:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"rewards (before)   -0.721547\nrewards (after)     0.461635\ndtype: float64"},"metadata":{}},{"name":"stdout","text":"\nmedian:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"rewards (before)   -1.032124\nrewards (after)     0.180650\ndtype: float64"},"metadata":{}}]},{"cell_type":"markdown","source":"**Как видно из полученной таблицы и расчета, в среднем число положительных результатов заметно выросло.**","metadata":{}},{"cell_type":"code","source":"def token_entropy(generations, tokenizer):\n    stats = defaultdict(int)\n    num_tokens = 0\n    for example in generations:\n        tokens = tokenizer.encode(example)\n        for t in tokens:\n            if t == tokenizer.pad_token_id:\n                continue\n            stats[t] += 1\n            num_tokens += 1\n    for k in stats.keys():\n        stats[k] /= num_tokens\n\n    return entropy(list(stats.values()))","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:27:49.500651Z","iopub.execute_input":"2023-12-11T09:27:49.500928Z","iopub.status.idle":"2023-12-11T09:27:49.508975Z","shell.execute_reply.started":"2023-12-11T09:27:49.500904Z","shell.execute_reply":"2023-12-11T09:27:49.507761Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"def create_test_generations(model, tokenizer):\n    generator = pipeline('text-generation', model=model, device=device, tokenizer=tokenizer, **generation_pipe_kwargs)\n    generated_reviews = generator(test_dataset['prompt'].to_list(), **generation_pipe_kwargs)\n    generated_texts = []\n    for batch_elem in tqdm(generated_reviews):\n        for x in batch_elem:\n            generated_texts.append(x['generated_text'])\n            break\n    return generated_texts","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:27:49.510449Z","iopub.execute_input":"2023-12-11T09:27:49.510801Z","iopub.status.idle":"2023-12-11T09:27:49.524197Z","shell.execute_reply.started":"2023-12-11T09:27:49.510768Z","shell.execute_reply":"2023-12-11T09:27:49.523568Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"hinge_token_entropy_before = token_entropy(test_dataset['chosen'], gpt2_tokenizer)\nhinge_token_entropy_after = token_entropy(create_test_generations(gpt2_model, gpt2_tokenizer), gpt2_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:27:49.525410Z","iopub.execute_input":"2023-12-11T09:27:49.526304Z","iopub.status.idle":"2023-12-11T09:29:47.125050Z","shell.execute_reply.started":"2023-12-11T09:27:49.526270Z","shell.execute_reply":"2023-12-11T09:29:47.123881Z"},"trusted":true},"execution_count":34,"outputs":[{"name":"stderr","text":"100%|██████████| 250/250 [00:00<00:00, 256626.53it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"hinge_token_entropy_df = pd.DataFrame({'token_entropy_before': [hinge_token_entropy_before], 'token_entropy_after': [hinge_token_entropy_after]})\nhinge_token_entropy_df","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:29:47.126676Z","iopub.execute_input":"2023-12-11T09:29:47.127262Z","iopub.status.idle":"2023-12-11T09:29:47.138920Z","shell.execute_reply.started":"2023-12-11T09:29:47.127221Z","shell.execute_reply":"2023-12-11T09:29:47.137748Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"   token_entropy_before  token_entropy_after\n0              6.371303             5.938285","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token_entropy_before</th>\n      <th>token_entropy_after</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6.371303</td>\n      <td>5.938285</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**После расчета энтропии, можно сказать, что она, ожидаемо, снизилась, поскольку модель стала делать предсказания в сторону положительных ответов.**","metadata":{}},{"cell_type":"markdown","source":"Заменим функцию потерь на sigmoid и обучем модель.","metadata":{}},{"cell_type":"code","source":"new_gpt2_model = AutoModelForCausalLM.from_pretrained(model_name)\nnew_gpt2_ref_model = AutoModelForCausalLM.from_pretrained(model_name)\nnew_gpt2_tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='left', return_tensors='pt')\n\nnew_gpt2_tokenizer.pad_token = new_gpt2_tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:29:47.140246Z","iopub.execute_input":"2023-12-11T09:29:47.141120Z","iopub.status.idle":"2023-12-11T09:29:51.697402Z","shell.execute_reply.started":"2023-12-11T09:29:47.141080Z","shell.execute_reply":"2023-12-11T09:29:51.696281Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"sigmoid_training_args = TrainingArguments(\n        per_device_train_batch_size=train_kwargs['per_device_train_batch_size'],\n        dataloader_num_workers=8,\n        max_steps=train_kwargs['max_steps'],\n        remove_unused_columns=False,\n        gradient_accumulation_steps=train_kwargs['gradient_accumulation_steps'],\n        learning_rate=train_kwargs['learning_rate'],\n        evaluation_strategy='steps',\n        logging_first_step=True,\n        logging_steps=500,\n        eval_steps=500,\n        per_device_eval_batch_size=16,\n        output_dir='./test2',\n        optim=\"rmsprop\",\n        warmup_steps=100,\n        report_to=train_kwargs['report_to'],\n        save_steps=train_kwargs['max_steps'],\n        gradient_checkpointing=False,\n    )","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:29:51.698621Z","iopub.execute_input":"2023-12-11T09:29:51.698910Z","iopub.status.idle":"2023-12-11T09:29:51.706549Z","shell.execute_reply.started":"2023-12-11T09:29:51.698883Z","shell.execute_reply":"2023-12-11T09:29:51.705505Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"sigmoid_dpo_trainer = DPOTrainer(\n        new_gpt2_model,\n        new_gpt2_ref_model,\n        args=sigmoid_training_args,\n        beta=train_kwargs['beta'],\n        train_dataset=train_dataset,\n        eval_dataset=eval_dataset,\n        tokenizer=gpt2_tokenizer,\n        max_length=train_kwargs['max_length'],\n        max_target_length=train_kwargs['max_target_length'],\n        max_prompt_length=train_kwargs['max_prompt_length'],\n        generate_during_eval=False,\n        loss_type='sigmoid'\n    )","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:29:51.707830Z","iopub.execute_input":"2023-12-11T09:29:51.708289Z","iopub.status.idle":"2023-12-11T09:29:51.986780Z","shell.execute_reply.started":"2023-12-11T09:29:51.708261Z","shell.execute_reply":"2023-12-11T09:29:51.985678Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"del gpt2_model, gpt2_ref_model, gpt2_tokenizer\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:29:51.988336Z","iopub.execute_input":"2023-12-11T09:29:51.988804Z","iopub.status.idle":"2023-12-11T09:29:52.443876Z","shell.execute_reply.started":"2023-12-11T09:29:51.988763Z","shell.execute_reply":"2023-12-11T09:29:52.442858Z"},"trusted":true},"execution_count":39,"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"93"},"metadata":{}}]},{"cell_type":"code","source":"sigmoid_dpo_trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:29:52.445036Z","iopub.execute_input":"2023-12-11T09:29:52.445381Z","iopub.status.idle":"2023-12-11T09:40:37.337366Z","shell.execute_reply.started":"2023-12-11T09:29:52.445343Z","shell.execute_reply":"2023-12-11T09:40:37.336297Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  warnings.warn(_create_warning_msg(\nCould not estimate the number of tokens of the input, floating-point operations will not be computed\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1000/1000 10:43, Epoch 4/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rewards/chosen</th>\n      <th>Rewards/rejected</th>\n      <th>Rewards/accuracies</th>\n      <th>Rewards/margins</th>\n      <th>Logps/rejected</th>\n      <th>Logps/chosen</th>\n      <th>Logits/rejected</th>\n      <th>Logits/chosen</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.382200</td>\n      <td>0.820190</td>\n      <td>-16.149477</td>\n      <td>-22.664883</td>\n      <td>0.814400</td>\n      <td>6.515410</td>\n      <td>-292.269928</td>\n      <td>-224.947250</td>\n      <td>-24.842876</td>\n      <td>-24.644238</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.006600</td>\n      <td>0.973736</td>\n      <td>-24.376251</td>\n      <td>-34.189003</td>\n      <td>0.833397</td>\n      <td>9.812756</td>\n      <td>-407.511139</td>\n      <td>-307.214996</td>\n      <td>-22.337610</td>\n      <td>-22.078920</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"execution_count":40,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1000, training_loss=0.19472437715530397, metrics={'train_runtime': 644.534, 'train_samples_per_second': 24.824, 'train_steps_per_second': 1.552, 'total_flos': 0.0, 'train_loss': 0.19472437715530397, 'epoch': 4.0})"},"metadata":{}}]},{"cell_type":"code","source":"sigmoid_dpo_trainer.save_model()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:40:37.339067Z","iopub.execute_input":"2023-12-11T09:40:37.339447Z","iopub.status.idle":"2023-12-11T09:40:38.187539Z","shell.execute_reply.started":"2023-12-11T09:40:37.339412Z","shell.execute_reply":"2023-12-11T09:40:38.186372Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"bs = len(test_dataset) // 10\nsigmoid_game_data = dict()\ntest_dataset.set_format('pandas')\ndf_sigmoid_batch = test_dataset[:].sample(bs)\nsigmoid_game_data['prompt'] = df_sigmoid_batch['prompt'].tolist()\nquery_tensors = df_sigmoid_batch['prompt'].map(new_gpt2_tokenizer).tolist()\noutput_min_length = 32\noutput_max_length = 64\noutput_length_sampler = LengthSampler(output_min_length, output_max_length)\nresponse_tensors_ref, response_tensors = [], []\n\nfor i in tqdm(range(bs)):\n    gen_len = output_length_sampler()\n    output = new_gpt2_ref_model.generate(\n        torch.tensor(query_tensors[i]['input_ids']).unsqueeze(dim=0).to(device), **generation_pipe_kwargs).squeeze()[-gen_len:]\n    response_tensors_ref.append(output)\n    output = new_gpt2_model.generate(\n        torch.tensor(query_tensors[i]['input_ids']).unsqueeze(dim=0).to(device), **generation_pipe_kwargs).squeeze()[-gen_len:]\n    response_tensors.append(output)\n\nsigmoid_game_data['response (before)'] = [new_gpt2_tokenizer.decode(response_tensors_ref[i]) for i in range(bs)]\nsigmoid_game_data['response (after)'] = [new_gpt2_tokenizer.decode(response_tensors[i]) for i in range(bs)]\n\nsigmoid_texts = [q + r for q, r in zip(sigmoid_game_data['prompt'], sigmoid_game_data['response (before)'])]\nsigmoid_game_data['rewards (before)'] = [output[0]['score'] if output[0]['label'] == 'POSITIVE' else output[1]['score'] for output in sentiment_pipe(sigmoid_texts,  **sentiment_pipe_kwargs)]\n\nsigmoid_texts = [q + r for q, r in zip(sigmoid_game_data['prompt'], sigmoid_game_data['response (after)'])]\nsigmoid_game_data['rewards (after)'] = [output[0]['score'] if output[0]['label'] == 'POSITIVE' else output[1]['score'] for output in sentiment_pipe(sigmoid_texts,  **sentiment_pipe_kwargs)]\n\ndf_sigmoid_results = pd.DataFrame(sigmoid_game_data)\ndf_sigmoid_results.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:42:47.017741Z","iopub.execute_input":"2023-12-11T09:42:47.018110Z","iopub.status.idle":"2023-12-11T09:43:11.926233Z","shell.execute_reply.started":"2023-12-11T09:42:47.018079Z","shell.execute_reply":"2023-12-11T09:43:11.924484Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stderr","text":"100%|██████████| 25/25 [00:24<00:00,  1.01it/s]\n/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n  warnings.warn(\n","output_type":"stream"},{"execution_count":46,"output_type":"execute_result","data":{"text/plain":"                                              prompt  \\\n0  <|startoftext|>You are not!!!<br /><br />I wat...   \n1  <|startoftext|> the show with the window set t...   \n2  <|startoftext|>http://eng.vezda.com/schutz/cel...   \n3  <|startoftext|><br /><br />Mickey, not to be f...   \n4  <|startoftext|> partying over mountain suds he...   \n\n                                   response (before)  \\\n0   animation since it is that she produced and i...   \n1  |> the show with the window set to \"film\" or \"...   \n2   payed 10$ for this one Polanski, wasn't one o...   \n3  br />Mickey, not to be forgotten, decided to l...   \n4   suds he takes tasteries to a small middle con...   \n\n                                    response (after)  rewards (before)  \\\n0  , my me Fwww!! smart EdIYou love://fi/&://wwwY...          2.375863   \n1  |> the show with the window set to \"film\" or \"...         -0.688315   \n2  /it myRAé8Br://You Good& my mylyULAllfi Ed- it...         -1.076934   \n3  br />Mickey, not to be forgotten, decided to l...         -0.189207   \n4   suds he takes til/\")ionag:// his day my- Good...          0.312062   \n\n   rewards (after)  \n0         0.840934  \n1         0.878340  \n2         0.608403  \n3         1.147862  \n4         1.402104  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prompt</th>\n      <th>response (before)</th>\n      <th>response (after)</th>\n      <th>rewards (before)</th>\n      <th>rewards (after)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>&lt;|startoftext|&gt;You are not!!!&lt;br /&gt;&lt;br /&gt;I wat...</td>\n      <td>animation since it is that she produced and i...</td>\n      <td>, my me Fwww!! smart EdIYou love://fi/&amp;://wwwY...</td>\n      <td>2.375863</td>\n      <td>0.840934</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>&lt;|startoftext|&gt; the show with the window set t...</td>\n      <td>|&gt; the show with the window set to \"film\" or \"...</td>\n      <td>|&gt; the show with the window set to \"film\" or \"...</td>\n      <td>-0.688315</td>\n      <td>0.878340</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>&lt;|startoftext|&gt;http://eng.vezda.com/schutz/cel...</td>\n      <td>payed 10$ for this one Polanski, wasn't one o...</td>\n      <td>/it myRAé8Br://You Good&amp; my mylyULAllfi Ed- it...</td>\n      <td>-1.076934</td>\n      <td>0.608403</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>&lt;|startoftext|&gt;&lt;br /&gt;&lt;br /&gt;Mickey, not to be f...</td>\n      <td>br /&gt;Mickey, not to be forgotten, decided to l...</td>\n      <td>br /&gt;Mickey, not to be forgotten, decided to l...</td>\n      <td>-0.189207</td>\n      <td>1.147862</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>&lt;|startoftext|&gt; partying over mountain suds he...</td>\n      <td>suds he takes tasteries to a small middle con...</td>\n      <td>suds he takes til/\")ionag:// his day my- Good...</td>\n      <td>0.312062</td>\n      <td>1.402104</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print('mean:')\ndisplay(df_sigmoid_results[['rewards (before)', 'rewards (after)']].mean())\nprint()\nprint('median:')\ndisplay(df_sigmoid_results[['rewards (before)', 'rewards (after)']].median())","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:43:15.647920Z","iopub.execute_input":"2023-12-11T09:43:15.648868Z","iopub.status.idle":"2023-12-11T09:43:15.670744Z","shell.execute_reply.started":"2023-12-11T09:43:15.648831Z","shell.execute_reply":"2023-12-11T09:43:15.669581Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"mean:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"rewards (before)   -0.373826\nrewards (after)     1.069654\ndtype: float64"},"metadata":{}},{"name":"stdout","text":"\nmedian:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"rewards (before)   -0.688315\nrewards (after)     0.976978\ndtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"sigmoid_token_entropy_before = token_entropy(test_dataset['chosen'], new_gpt2_tokenizer)\nsigmoid_token_entropy_after = token_entropy(create_test_generations(new_gpt2_model, new_gpt2_tokenizer), new_gpt2_tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:43:18.193183Z","iopub.execute_input":"2023-12-11T09:43:18.194005Z","iopub.status.idle":"2023-12-11T09:45:22.317009Z","shell.execute_reply.started":"2023-12-11T09:43:18.193970Z","shell.execute_reply":"2023-12-11T09:45:22.315872Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stderr","text":"100%|██████████| 250/250 [00:00<00:00, 250376.31it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"sigmoid_token_entropy_df = pd.DataFrame({'token_entropy_before': [sigmoid_token_entropy_before], 'token_entropy_after': [sigmoid_token_entropy_after]})\nsigmoid_token_entropy_df","metadata":{"execution":{"iopub.status.busy":"2023-12-11T09:46:25.398387Z","iopub.execute_input":"2023-12-11T09:46:25.398780Z","iopub.status.idle":"2023-12-11T09:46:25.411823Z","shell.execute_reply.started":"2023-12-11T09:46:25.398750Z","shell.execute_reply":"2023-12-11T09:46:25.410701Z"},"trusted":true},"execution_count":51,"outputs":[{"execution_count":51,"output_type":"execute_result","data":{"text/plain":"   token_entropy_before  token_entropy_after\n0              6.371303             5.287679","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token_entropy_before</th>\n      <th>token_entropy_after</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>6.371303</td>\n      <td>5.287679</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"### **Вывод:**  \n   В ходе проведения первой части экспериментов был собран датасет winner-loser размером 5000, затем на нем были обучены модели при помощи DPOTrainer, получились следующие результаты:\n1. При обучении с hinge loss:\n    * средняя награда выросла с -0.721547 до 0.461635, а медианная - с -1.032124 до 0.180650\n    * энтропия снизилась с 6.371303 до 5.938285\n2. При обучении с sigmoid loss:\n    * средняя награда выросла с -0.373826 до 1.069654, а медианная - с -0.688315 до 0.976978 \n    * энтропия снизилась с 6.371303 до 5.287679   \n    \nПолученные данные объясняются тем, что модель обучалась, и как итог, стала приближать свои ответ к положительному классу, то есть выполнять то, чего мы и добивались. Как видно из показателей, при обучении с функцией потерь sigmoid loss, модель обучалась лучше, поэтому при дообучении модели, используя DPOTrainer \"из коробки\" оптимально использовать её. Можно ли было бы улучшить имеющиеся значения? Возможно, как вариант - увеличение размера выборки и эпох обучения, но при обучении на бесплатных ресурасах (колаб, каггл) этого тяжело добиться, поскольку ресурсы ограничены.","metadata":{}}]}